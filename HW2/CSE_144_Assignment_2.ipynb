{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE-144-Assignment-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 144 Winter 2022 Assignment 2\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel→→Restart) and then run all cells (in the menubar, select Cell→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ],
      "metadata": {
        "id": "R3SW8S-xw-ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NAME = \"\"\n",
        "STUDENT_ID = \"\""
      ],
      "metadata": {
        "id": "Q0Q-KKeGHoy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Linear Regression On Non-Linear Data (Total: 50 points)\n",
        "\n",
        "In this question, you will be implementing the linear regression algorithm from scratch in Python. As you know, linear regression aims to map feature vectors to a continuous value in the range $[-\\infty,+\\infty]$ by linearly combining the feature values.\n",
        "\n",
        "### Model Representation\n",
        "As you have seen previously in assignment 1, we represent our data as a dataframe or a feature matrix. \n",
        "\n",
        "Let our feature matrix be $X$ whose dimensions are $n \\times m$, $\\theta$ be a weight vector of dimensions $m \\times 1$, the bias vector $b$ a vector of dimension $n\\times 1$. Using these we can predict $\\hat{Y}$\n",
        "by the following relationship:\n",
        "\n",
        "$$\\hat{Y} = X\\theta + b$$\n",
        "\n",
        "(Does this look familiar? Remember $y = mx + b$)\n",
        "\n",
        "This is equivalent to\n",
        "\n",
        "$$\\hat{Y} = X\\theta \\text{, where } \\theta\\in \\mathbb{R^{m+1}},X\\in\\mathbb{R^{n\\times m+1}}$$ The bias trick was applied here.\n",
        "\n",
        "### Data: \n",
        "\n",
        "We generate random points and use them to create a polynomial.\n",
        "\n"
      ],
      "metadata": {
        "id": "qzFKaarDnTsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting non-linear data\n",
        "\n",
        "Data does not always follow a linear relationship from the independent variable $X$ to the dependent variable $y$. Fitting a linear model to this would be inaccurate and yield a high loss. \n",
        "\n",
        "If we want to model an order $d$ polynomial relationship between $X$ and $y$ we can augment our initial linear model where instead of having:\n",
        "$$\n",
        "y_i = \\theta_0 + \\theta_1 x_i\n",
        "$$\n",
        "\n",
        "We have:\n",
        "\n",
        "$$\n",
        "y_i = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\cdots + \\theta_d x_i^d\n",
        "$$\n",
        "\n",
        "We can use the same linear regression algorithm we if we first augment $X$ and add extra columns (or dimensions). \n",
        "\n",
        "$$ \\textbf X =\n",
        "\\begin{bmatrix}\n",
        "    x_{1}       & x_{1}^2 & \\cdots & x_{1}^d \\\\\n",
        "    x_{2}       & x_{2}^2 & \\cdots & x_{2}^d \\\\\n",
        "    \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{n}       & x_{n}^2 & \\cdots & x_{n}^d\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Then our new higher order $\\hat y$ is computed same as before.\n",
        "\n",
        "$$ \\hat y =  X \\theta =\n",
        "\\begin{bmatrix}\n",
        "    1 & x_{1}       & x_{1}^2 & \\cdots & x_{1}^d \\\\\n",
        "    1 & x_{2}       & x_{2}^2 & \\cdots & x_{2}^d \\\\\n",
        "    \\vdots & \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{n}       & x_{n}^2 & \\cdots & x_{n}^d\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_{d} \n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "    \\theta_0 + \\theta_1 x_{1} + \\theta_2 x_{1}^2 + \\cdots + \\theta_{d}  x_{1}^d \\\\\n",
        "    \\theta_0 + \\theta_1 x_{2} + \\theta_2 x_{2}^2 + \\cdots + \\theta_{d}  x_{2}^d  \\\\\n",
        "    \\vdots   \\\\\n",
        "    \\theta_0 + \\theta_1 x_{n} + \\theta_2 x_{n}^2 + \\cdots + \\theta_{d}  x_{n}^d\n",
        "\\end{bmatrix} \n",
        "= \\begin{bmatrix}\\hat y_1 \\\\ \\hat y_2 \\\\ \\vdots \\\\ \\hat y_{n} \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4dGTshtLqgND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the data"
      ],
      "metadata": {
        "id": "FqJlLmQtnjCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def normalize_data(data):\n",
        "    return (data - np.min(data))/(np.max(data) - np.min(data))\n",
        "\n",
        "np.random.seed(33)\n",
        "x = np.random.uniform(-10, 10, 1000)\n",
        "poly_coeffs = np.random.uniform(-1,1, size=(4,1))\n",
        "y = poly_coeffs[0] + poly_coeffs[1]*x + poly_coeffs[2]*(x ** 2) + poly_coeffs[3]*(x ** 3) + np.random.normal(0, 250, 1000)\n",
        "\n",
        "x2 = np.random.uniform(-10, 10, 1000)\n",
        "poly_coeffs = np.random.uniform(-1,1, size=(3,1))\n",
        "y2 = poly_coeffs[0] - 2000 + poly_coeffs[1]*x2 + 50*poly_coeffs[2]*(x2 ** 2)  + np.random.normal(0, 250, 1000)\n",
        "\n",
        "x = np.concatenate([x,x2])\n",
        "y = np.concatenate([y,y2])\n",
        "x = normalize_data(x)\n",
        "y = normalize_data(y)\n",
        "\n",
        "plt.scatter(x,y, s=10)\n",
        "plt.show()\n",
        "\n",
        "poly_data = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\n",
        "np.random.shuffle(poly_data)\n",
        "x = poly_data[:,0]\n",
        "y = poly_data[:,1]"
      ],
      "metadata": {
        "id": "JKNO9Ug9nS8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "AsAyG6VIp_85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = LinearRegression().fit(x.reshape(-1,1), y)"
      ],
      "metadata": {
        "id": "nt57S-KDqDcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_line_from_regr(X_data, y_data, regr):\n",
        "    l_bound = np.min(X_data)\n",
        "    r_bound = np.max(X_data)\n",
        "    return [l_bound, r_bound], [l_bound * regr.coef_ + regr.intercept_, r_bound * regr.coef_ + regr.intercept_]\n",
        "\n",
        "plt.scatter(x,y, s=10)\n",
        "line_x, line_y = compute_line_from_regr(x.reshape(-1,1),y,reg)\n",
        "plt.plot(line_x, line_y, color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zoURctooqEzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see above, this data doesn't follow a linear relationship, it follows some complex polynomial. In the next section you'll try to fit a higher degree polynomial to it.\n",
        "\n",
        "\n",
        "## Weight regularization\n",
        "When we try to fit a d-order polynomial to our data, we could end up overfitting. This happens when you try to fit a higher dimensional curve than what the distribution of our data actually exhibits. We can mitigate this by choosing an order $d$ that matches your data closely, but often times this is not directly apperant in noisy data. Another method to avoid overfitting is **regularizing**, where you modify your loss to keep weights small which flattens our polynomial. This helps us avoid learning polynomials that are too complex for our data.\n",
        "\n",
        "To add regularization we modify our original loss function $J$ to include our regularizing term and a new hyperparameter that we tune $\\lambda$. This controls the amount of regularizing we impose on the weights. We use the loss computed from the validation set to tweak this parameter.\n",
        "\n",
        "$$\n",
        "J(\\theta)=\\frac{1}{2n}\\sum^{n}_{i=1}(h^{(i)}-y^{(i)})^2 + \\lambda \\sum^{d}_{j=1} \\theta^2_j\n",
        "$$\n",
        "\n",
        "Our gradient computation also changes:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} ( h^{(i)}-y^{(i)})x_{i,j}  + 2 \\lambda\\theta_j \n",
        "$$\n",
        "\n",
        "We apply this gradient the same way as before in our gradient descent algorithm:\n",
        "$$\n",
        " \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
        "$$"
      ],
      "metadata": {
        "id": "BwZTPDBkqKca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(a) Train-Val-Test Split - 10 points"
      ],
      "metadata": {
        "id": "8sYDHEdTnpig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bias_trick(X):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "def separate_data(data):\n",
        "    # Split into X (remember to use bias trick) and Y\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "\n",
        "'''\n",
        "Takes raw data in and splits the data into \n",
        "X_train, y_train, X_test, y_test, X_val, y_val\n",
        "Returns X_train, y_train, X_test, y_test, X_val, y_val\n",
        "'''\n",
        "def train_test_validation_split(data, test_size=.20, validation_size=.20):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE =========="
      ],
      "metadata": {
        "id": "k_BwzSRJnIgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) L2 Regularization - 40 points\n",
        "\n",
        "Implement:\n",
        "\n",
        "* ```mse``` - 5 points\n",
        "* ```mse_derivative``` - 5 points\n",
        "* ```l2norm``` - 5 points\n",
        "* ```l2norm_derivative``` - 5 points\n",
        "* ```compute_cost``` - 5 points\n",
        "* ```gradient_descent_step``` - 5 points\n",
        "* ```polynomial_regression``` - 10 points\n"
      ],
      "metadata": {
        "id": "9IkOAyeBoiPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Adds columns to your data up to the specified degree.\n",
        "Ex: If degree=3, (x) -> (x, x^2, x^3)\n",
        "'''\n",
        "def add_polycols(X,degree):\n",
        "    x_col = X[:,-1]\n",
        "\n",
        "    for i in range(2, degree+1):\n",
        "        X = np.hstack((X,(x_col**i).reshape(-1,1)))\n",
        "    return X\n",
        "\n",
        "'''\n",
        "Takes the target values and predicted values and calculates the absolute error \n",
        "between them \n",
        "'''\n",
        "def mse(y_pred, y_true):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "'''\n",
        "Implementation of the derivative of MSE.\n",
        "Returns a vector of derivations of loss with respect to each of the dimensions\n",
        "[\\partial loss / \\partial \\theta_i]\n",
        "'''\n",
        "def mse_derivative(X,y,theta):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "'''\n",
        "Computes L2 norm from theta and lambda.\n",
        "Returns a scalar l2norm.\n",
        "'''\n",
        "def l2norm(theta, lamb):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "'''\n",
        "Computes derivative of L2 norm.\n",
        "Returns a vector of derivative of L2 norms.\n",
        "'''\n",
        "def l2norm_derivative(theta, lamb):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # Note there is no regularization on the bias term.\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "'''\n",
        "Computes total cost (cost function + regularization term)\n",
        "'''\n",
        "def compute_cost(X, y, theta, lamb):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "'''\n",
        "Gradient descent step. \n",
        "Takes X, y, theta vector, and alpha. \n",
        "Returns an updated theta vector.\n",
        "'''\n",
        "def gradient_descent_step(X, y, theta, alpha, lamb):\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "\n",
        "def polynomial_regression(data, degree, num_epochs=100000, alpha=1e-4, lamb=0):\n",
        "    # Get training, testing, and validation sets by calling train_test_validation_split()\n",
        "    # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "    # Record training and validation errors in lists\n",
        "    train_errors = []\n",
        "    val_errors = []\n",
        "\n",
        "    # Add the appropriate amount of columns to each of your sets of data.\n",
        "    \n",
        "\n",
        "    # Define theta\n",
        "    # Carry out training loop\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        # Do gradient descent on the training set\n",
        "        # This prints the validation loss\n",
        "        if i % (num_epochs//10) == 0:\n",
        "            print(f'({i} epochs) Training loss: {train_error}, Validation loss: {val_error}') \n",
        "    print(f'({i} epochs) Final training loss: {train_error}, Final validation loss: {val_error}') \n",
        "    \n",
        "    # Compute the testing loss\n",
        "    print(f'Final testing loss: {test_error}')\n",
        "    plt.plot(np.arange(num_epochs), train_errors, label=\"Train loss\")\n",
        "    plt.plot(np.arange(num_epochs), val_errors, label=\"Val loss\")\n",
        "    plt.title(\"Train + validation loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    return theta\n",
        "    # ========== YOUR CODE STARTS HERE =========="
      ],
      "metadata": {
        "id": "bobnVDWQow26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned above, we use the validation set's loss to tweak our hyperparameters. Please carry out the training task while monitoring the validation loss and varying the polynomial order $d$ and regularization constant $\\lambda$. Your answer should get close to minimizing the validation and testing losses. "
      ],
      "metadata": {
        "id": "7u38fYKGouDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# degree d\n",
        "polynomial_order = 10\n",
        "\n",
        "# regularization constant lambda\n",
        "regularization_param = 0.10\n",
        "num_epochs=40000\n",
        "theta = polynomial_regression(poly_data, polynomial_order, lamb=regularization_param, num_epochs=num_epochs, alpha=1e-4)"
      ],
      "metadata": {
        "id": "hKvWoQNEsPpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call plot_results() to see how your polynomial fits.\n",
        "def plot_results(theta, X, Y):\n",
        "    y_hat = sum([t*X**i for i,t in enumerate(theta)])\n",
        "    plt.scatter(X, y_hat, s=1, color='r')\n",
        "    plt.scatter(X, Y, s=1)\n",
        "    plt.show()\n",
        "plot_results(theta, x, y)"
      ],
      "metadata": {
        "id": "IDiTud6BsRBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 Logistic Regression - (Total: 50 points)\n",
        "##Dataset preparation\n",
        "In the cell below, you will read training and test data. You should split the dataset into features and labels for each of training, validation, and test sets. "
      ],
      "metadata": {
        "id": "DF5DHsvoaqik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "rm *.csv*\n",
        "wget -nv https://storage.googleapis.com/cse144/{train,test}.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlC51pqGgoS6",
        "outputId": "c4b6bb00-95c4-471e-b928-e216d18b568e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.csv*': No such file or directory\n",
            "2022-04-17 18:42:35 URL:https://storage.googleapis.com/cse144/train.csv [32902/32902] -> \"train.csv\" [1]\n",
            "2022-04-17 18:42:35 URL:https://storage.googleapis.com/cse144/test.csv [8270/8270] -> \"test.csv\" [1]\n",
            "FINISHED --2022-04-17 18:42:35--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 2 files, 40K in 0.001s (52.2 MB/s)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format=\"retina\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def plot_data(x: np.ndarray, y: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Plot a dataset with 2-d feature vectors and binary labels. \n",
        "\n",
        "    Args:\n",
        "        x: 2-d feature vectors\n",
        "        y: 1-d binary labels.\n",
        "    \"\"\"\n",
        "    class0_idx = np.where(y == 0)[0]\n",
        "    class1_idx = np.where(y == 1)[0]\n",
        "    feature0 = x[:, 0]\n",
        "    feature1 = x[:, 1]\n",
        "    plt.scatter(feature0[class0_idx], feature1[class0_idx], label=\"0\")\n",
        "    plt.scatter(feature0[class1_idx], feature1[class1_idx], label=\"1\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_decision_boundary(theta, x) -> None:\n",
        "    \"\"\"\n",
        "    Plot the decision boundary using theta. Use this function with plot_data().\n",
        "\n",
        "    Args:\n",
        "        theta: a 3-d weight vector.\n",
        "        x: 2-d feature vectors, which is used to decide the span of the decision\n",
        "           boundary.\n",
        "    \"\"\"\n",
        "    xx = np.linspace(min(x[:, 0]), max(x[:, 0]))\n",
        "    yy = (-theta[1] / theta[2]) * xx - (theta[0]) / theta[2]\n",
        "    plt.plot(xx, yy, color=\"red\", label=\"boundary\")\n",
        "    plt.ylim(min(x[:, 1]), max(x[:, 1]))\n",
        "\n",
        "\n",
        "# Read datasts and split your training data into train & validation sets. Split\n",
        "# features from labels after that.\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "# ========== YOUR CODE ENDS HERE ==========\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_val.shape, y_val.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "O3qPeXtUw76-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot training and validation sets"
      ],
      "metadata": {
        "id": "p9uz4oGuau6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(x_train, y_train)\n",
        "plot_data(x_val, y_val)"
      ],
      "metadata": {
        "id": "pQ_9N-r9a7Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression\n",
        "You'll complete the missing parts in the eight functions in the `LogisticRegressionTrainer` class below. Note that you are not supposed to return anything in `gradient_descent_step()` but update the parameters. Especially, do not forget to add the regularization term in `cross_entropy_loss()`.\n",
        "\n",
        "TODO:\n",
        "* initialize ```theta``` - 5 points\n",
        "* ```gradient_descent_step``` - 5 points\n",
        "* ```sigmoid``` - 5 points\n",
        "* ```cross_entropy_loss``` - 5 points\n",
        "* ```cross_entropy_loss_derivative``` - 5 points\n",
        "* ```accuracy``` - 5 points\n",
        "* ```train``` - 5 points\n",
        "* ```evaluate``` - 5 points\n",
        "* Use Scikit learn - 10 points "
      ],
      "metadata": {
        "id": "pqOAWFA_axTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_features: int,\n",
        "        learning_rate: float = 1e-2,\n",
        "        num_epochs: int = 500,\n",
        "        lambd: float = 0.0,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize a logisitc regression trainer.\"\"\"\n",
        "        self.lambd = lambd\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_features = num_features\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "        self.test_loss = None\n",
        "        self.test_acc = None\n",
        "\n",
        "        # Initialize weights for your model. You can use any initialization methods.\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def gradient_descent_step(self, x: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Perform a single step of gradient update.\n",
        "\n",
        "        Args:\n",
        "            x: A matrix of features.\n",
        "            y: A vector of labels.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert raw model output (logits) to probabilities.\n",
        "\n",
        "        Args:\n",
        "            z: Raw model output (logits).\n",
        "\n",
        "        Returns:\n",
        "            A vector (or float, if your input is a scalar) of probabilties.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def cross_entropy_loss(self, pred: np.ndarray, target: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the binary cross-entropy loss given predictions and targets.\n",
        "        The loss function should add the regularization term.\n",
        "\n",
        "        Args:\n",
        "            pred: Predicted labels (probabilities).\n",
        "            target: Ground-truth labels.\n",
        "\n",
        "        Returns:\n",
        "            A scalar of loss.\n",
        "        \"\"\"\n",
        "        assert pred.shape == target.shape\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def cross_entropy_loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the derivative of the loss function w.r.t. theta. The derivative of the\n",
        "        loss function should also add the derivative of the L2 regularization term.\n",
        "\n",
        "        Args:\n",
        "            x: Feature vectors.\n",
        "            y: Ground-truth labels.\n",
        "\n",
        "        Returns:\n",
        "            A vector with the same dimension as theta, where each element is the\n",
        "            partial derivative of the loss function w.r.t. the corresponding element\n",
        "            in theta.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def accuracy(self, pred: np.ndarray, target: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the percentage of matched labels given predictions and targets.\n",
        "\n",
        "        Args:\n",
        "            pred: Predicted labels (rounded probabilities).\n",
        "            target: Ground-truth labels.\n",
        "\n",
        "        Return:\n",
        "            The accuracy score (a float) given the predicted labels and the true labels.\n",
        "        \"\"\"\n",
        "        assert pred.shape == target.shape\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        x_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        x_val: np.ndarray,\n",
        "        y_val: np.ndarray,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Run gradient descent for n epochs, where n = self.num_epochs. In every epoch,\n",
        "            1. Update theta.\n",
        "            2. Calculate the training loss & accuracy given the current theta, and append \n",
        "               then to self.train_loss_history and self.train_acc_history.\n",
        "            3. Calculate the validation loss & accuracy given the current theta, and \n",
        "               append then to self.train_loss_history and self.train_acc_history.\n",
        "\n",
        "        If you wish to use the bias trick, please remember to use it before the for loop.\n",
        "\n",
        "        Args:\n",
        "            x_train: Feature vectors for training.\n",
        "            y_train: Ground-truth labels for training.\n",
        "            x_val: Feature vectors for validation.\n",
        "            y_val: Ground-truth labels for validation.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # Do bias trick\n",
        "        x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
        "        x_val = np.hstack((np.ones((x_val.shape[0], 1)), x_val))\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Do a gradient descent step\n",
        "\n",
        "            # Calcuate train loss\n",
        "\n",
        "            # Calcuate test loss\n",
        "\n",
        "            # Calculate training accuracy\n",
        "  \n",
        "            #Calcuate validation accuracy\n",
        "\n",
        "            # Record train loss\n",
        "\n",
        "            # Record test loss\n",
        "\n",
        "            # Record training accuracy\n",
        "  \n",
        "            #Record validation accuracy\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Evaluate the model on test set and store the test loss int self.test_loss and \n",
        "        test accuracy in self.test_acc. In other words, you should get the test loss and accraucy here.\n",
        "\n",
        "        If you used the bias trick in train(), you have to also use it here.\n",
        "\n",
        "        Args:\n",
        "            x_test: Feature vectors for testing.\n",
        "            y_test: Ground-truth labels for testing.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        # Do bias trick\n",
        "        # Calculate test loss\n",
        "        # Record test accuracy\n",
        "        # ========== YOUR CODE ENDS HERE =========="
      ],
      "metadata": {
        "id": "YZOF6w9lw3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a logistic regression classifier"
      ],
      "metadata": {
        "id": "8DQ5HqSEazeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a logistic regression classifier\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "# ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "print(f\"Final train loss: {trainer.train_loss_history[-1]}\")\n",
        "print(f\"Final validation loss: {trainer.val_loss_history[-1]}\")\n",
        "print(f\"Final train acc: {trainer.train_acc_history[-1]}\")\n",
        "print(f\"Final validation acc: {trainer.val_acc_history[-1]}\")\n",
        "\n",
        "plt.plot(np.arange(trainer.num_epochs), trainer.train_loss_history, label=\"Train loss\")\n",
        "plt.plot(np.arange(trainer.num_epochs), trainer.val_loss_history, label=\"Val loss\")\n",
        "plt.title(\"Train & validation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(trainer.num_epochs), trainer.train_acc_history, label=\"Train acc\")\n",
        "plt.plot(np.arange(trainer.num_epochs), trainer.val_acc_history, label=\"Val acc\")\n",
        "plt.title(\"Train & validation acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1knVC-6KkTuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "6KEuYFqHa4LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate your model on the test set\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "# ========== YOUR CODE ENDS HERE ==========\n",
        "print(f\"Test loss: {trainer.test_loss}\")\n",
        "print(f\"Test acc: {trainer.test_acc}\")"
      ],
      "metadata": {
        "id": "ltBpOW90CFJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Sci-kit Learn\n",
        "\n",
        "Tasks\n",
        "\n",
        "* Import and instantiate a Logistic Regression Classifier\n",
        "* Fit the model to the training set\n",
        "* Report the training set, validation set and test set accuracy.\n",
        "* Visually compare this model to your previous model by plotting the decision boundary on the validation set. Comment on the coefficients learned."
      ],
      "metadata": {
        "id": "KVhOxL0N2DHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and make an instance of a Sklearn Logistic Regression Classifier \n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "# ========== YOUR CODE STARTS HERE =========="
      ],
      "metadata": {
        "id": "PsNphLCphvsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit your classifier to the training set you created previously.\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "# ========== YOUR CODE STARTS HERE =========="
      ],
      "metadata": {
        "id": "3mUVqrp92Izp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Report accuracy on the training, val and test set.\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "# ========== YOUR CODE STARTS HERE =========="
      ],
      "metadata": {
        "id": "zoU1fV61361o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting decision boundaries"
      ],
      "metadata": {
        "id": "lmXGufdLcTbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 = theta0 + theta1 * x + theta2 * y\n",
        "# y = (-theta0 - theta1 * x) / theta2\n",
        "print(f\"My logistic regression weights: {trainer.theta}\")\n",
        "plot_decision_boundary(trainer.theta, x_val)\n",
        "plot_data(x_val, y_val)\n",
        "\n",
        "print(f\"Sklearn logisitic regression weights: {np.append(lr_classifier.intercept_, lr_classifier.coef_)}\")\n",
        "plot_decision_boundary(np.append(lr_classifier.intercept_, lr_classifier.coef_), x_val)\n",
        "plot_data(x_val, y_val)"
      ],
      "metadata": {
        "id": "xSZgVez04KwY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}